{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "copDPOrWXG6i"
      },
      "source": [
        "## What You Need to Implement in This Hackathon:\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "\n",
        "1. Your own **Bot class** that includes:\n",
        "   - A `.act(player_info)` function that returns an action dictionary.\n",
        "   - Example: `{\"forward\": True, \"shoot\": False, \"rotate\": 0}`\n",
        "\n",
        "2. Your own **reward function**:\n",
        "   - You can modify `calculate_reward(self, info_dictionary, bot_username)`\n",
        "   - Use `bot_info` dictionary to understand what your agent did:\n",
        "     - `damage_dealt`, `kills`, `location`, `health`, etc.\n",
        "\n",
        "3. Optional: Tweak `frame_skip` to make training faster.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Eu5YHfqSUv38"
      },
      "source": [
        "# How to use the provided code"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 315
        },
        "id": "-6aKIGD_qRd9",
        "outputId": "d043e257-4c08-47d1-9bbc-219ffaf8d664"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "pygame 2.6.1 (SDL 2.28.4, Python 3.9.13)\n",
            "Hello from the pygame community. https://www.pygame.org/contribute.html\n"
          ]
        }
      ],
      "source": [
        "from Environment import Env "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Sg3EoiDNPOfB"
      },
      "source": [
        "Setup and use the environment"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 211
        },
        "id": "171T6TZkPPXB",
        "outputId": "33e99263-faa1-4ae8-8313-0fbdb7117d42"
      },
      "outputs": [
        {
          "ename": "NameError",
          "evalue": "name 'world_width' is not defined",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "Cell \u001b[1;32mIn[4], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m env \u001b[38;5;241m=\u001b[39m Env(training\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, \u001b[38;5;66;03m# if set to false the game will play with a simple UI\u001b[39;00m\n\u001b[0;32m      2\u001b[0m           use_game_ui\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, \u001b[38;5;66;03m# if set to True and training is set to false it will display the game with a advanced UI\u001b[39;00m\n\u001b[1;32m----> 3\u001b[0m           world_width\u001b[38;5;241m=\u001b[39m\u001b[43mworld_width\u001b[49m, \u001b[38;5;66;03m# do not change this\u001b[39;00m\n\u001b[0;32m      4\u001b[0m           world_height\u001b[38;5;241m=\u001b[39mworld_height, \u001b[38;5;66;03m# do not change this\u001b[39;00m\n\u001b[0;32m      5\u001b[0m           display_width\u001b[38;5;241m=\u001b[39mdisplay_width, \u001b[38;5;66;03m# do not change this\u001b[39;00m\n\u001b[0;32m      6\u001b[0m           display_height\u001b[38;5;241m=\u001b[39mdisplay_height, \u001b[38;5;66;03m# do not change this\u001b[39;00m\n\u001b[0;32m      7\u001b[0m           n_of_obstacles\u001b[38;5;241m=\u001b[39mn_of_obstacles, \u001b[38;5;66;03m# passes the number of obstacles\u001b[39;00m\n\u001b[0;32m      8\u001b[0m           frame_skip\u001b[38;5;241m=\u001b[39mconfig[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mframe_skip\u001b[39m\u001b[38;5;124m\"\u001b[39m]) \u001b[38;5;66;03m# number of frames to skip each step\u001b[39;00m\n",
            "\u001b[1;31mNameError\u001b[0m: name 'world_width' is not defined"
          ]
        }
      ],
      "source": [
        "env = Env(training=True, # if set to false the game will play with a simple UI\n",
        "          use_game_ui=False, # if set to True and training is set to false it will display the game with a advanced UI\n",
        "          world_width=world_width, # do not change this\n",
        "          world_height=world_height, # do not change this\n",
        "          display_width=display_width, # do not change this\n",
        "          display_height=display_height, # do not change this\n",
        "          n_of_obstacles=n_of_obstacles, # passes the number of obstacles\n",
        "          frame_skip=config[\"frame_skip\"]) # number of frames to skip each step"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b-p_9Xu7fuqD"
      },
      "source": [
        "Setup multiple characters competing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "niXirKnJf298"
      },
      "outputs": [],
      "source": [
        "players = [\n",
        "        Character(starting_pos=(world_bounds[2] - 100, world_bounds[3] - 100),\n",
        "                  screen=env.world_surface,\n",
        "                  boundaries=world_bounds,\n",
        "                  username=\"Ninja\"),\n",
        "        Character(starting_pos=(world_bounds[0] + 10, world_bounds[1] + 10),\n",
        "                  screen=env.world_surface,\n",
        "                  boundaries=world_bounds,\n",
        "                  username=\"Faze Jarvis\"),\n",
        "]\n",
        "\n",
        "# in this case 2 characters are created starting in opposite corners"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_eOz4uP-f8bQ"
      },
      "source": [
        "Each character can be controlled by a different bot"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nlPh_kA7f7zK"
      },
      "outputs": [],
      "source": [
        "bots = []\n",
        "    for _ in players:\n",
        "        bot = MyBot(action_size=config[\"action_size\"])\n",
        "        bot.use_double_dqn = config[\"hyperparameters\"][\"double_dqn\"]\n",
        "        bot.learning_rate = config[\"hyperparameters\"][\"learning_rate\"]\n",
        "        bot.batch_size = config[\"hyperparameters\"][\"batch_size\"]\n",
        "        bot.gamma = config[\"hyperparameters\"][\"gamma\"]\n",
        "        bot.epsilon_decay = config[\"hyperparameters\"][\"epsilon_decay\"]\n",
        "        bot.optimizer = torch.optim.Adam(bot.model.parameters(), lr=bot.learning_rate)\n",
        "        bots.append(bot)\n",
        "\n",
        "    # --- link players and bots to environment ---\n",
        "    env.set_players_bots_objects(players, bots)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4XgREhi8g37l"
      },
      "source": [
        "Then set the rewards dictionary to hold all the data for each character\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UWU2bvKVPQmN"
      },
      "outputs": [],
      "source": [
        "all_rewards = {player.username: [] for player in players}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9ZHzLvD1L8K6"
      },
      "source": [
        "# What methods CAN you modify\n",
        "### In the Enviroment class\n",
        "- `calculate_reward`\n",
        "\n",
        "### In the My_bot class\n",
        "- `All the code`\n",
        "\n",
        "### In the main script\n",
        "- `All the code`"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zfPK7QtOVqQA"
      },
      "source": [
        "# Main concept to understand\n",
        "You are generally allowed to modify the code how you think it fits best during the training phase but at the final tournament the code that will be used is the one initially provided by US, the only code that will be used from your implementations are the ones contained in the bots modules. This ensures that all bots utilize the same game environment and information."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dx_R_c0PO_OL"
      },
      "source": [
        "# Enviroment code"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NNu0qHDoUwuX"
      },
      "source": [
        "This environment is built using Pygame and provides a 2D simulation where bots can navigate, shoot, and interact. You'll be building a neural network that plays this game, and you'll design a reward function to guide its learning."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 419
        },
        "id": "Psgv4d36NuHW",
        "outputId": "ba35377b-6114-4ab4-fbec-1640d5a0d61e"
      },
      "outputs": [],
      "source": [
        "import math\n",
        "import os\n",
        "import pygame\n",
        "from components.advanced_UI import game_UI\n",
        "from components.world_gen import spawn_objects\n",
        "\n",
        "class Env:\n",
        "    def __init__(self, training=False, use_game_ui=True, world_width=1280, world_height=1280, display_width=640,\n",
        "                 display_height=640, n_of_obstacles=10, frame_skip=4):\n",
        "        pygame.init()\n",
        "\n",
        "        self.training_mode = training\n",
        "\n",
        "        ...\n",
        "\n",
        "        # INIT SOME VARIABLES\n",
        "        self.OG_bots = None\n",
        "        self.OG_players = None\n",
        "        self.OG_obstacles = None\n",
        "\n",
        "        self.bots = None\n",
        "        self.players = None\n",
        "        self.obstacles = None\n",
        "\n",
        "        \"\"\"REWARD VARIABLES\"\"\"\n",
        "        self.last_positions = {}\n",
        "        self.last_damage = {}\n",
        "        self.last_kills = {}\n",
        "        self.last_health = {}\n",
        "        self.visited_areas = {}\n",
        "\n",
        "        self.visited_areas.clear()\n",
        "        self.last_positions.clear()\n",
        "        self.last_health.clear()\n",
        "        self.last_kills.clear()\n",
        "        self.last_damage.clear()\n",
        "\n",
        "        self.steps = 0\n",
        "\n",
        "    def set_players_bots_objects(self, players, bots, obstacles=None):\n",
        "        ...\n",
        "        # sets players and bot in the class and then resets\n",
        "\n",
        "    def get_world_bounds(self):\n",
        "        ...\n",
        "        # returns (0, 0, self.world_width, self.world_height)\n",
        "\n",
        "    def reset(self, randomize_objects=False, randomize_players=False):\n",
        "        ...\n",
        "        # resets the variables and the environment\n",
        "\n",
        "    def step(self, debugging=False):\n",
        "\n",
        "        # frame skipping for training acceleration\n",
        "        skip_count = self.frame_skip if self.training_mode else 1\n",
        "\n",
        "        # placeholder for the variables\n",
        "        game_over = False\n",
        "        final_info = None\n",
        "\n",
        "        # get actions once and reuse them for all skipped frames\n",
        "        player_actions = {}\n",
        "        if self.training_mode:\n",
        "            for player in self.players:\n",
        "                if player.alive:\n",
        "                    player_info = player.get_info()\n",
        "                    player_info['closest_opponent'] = self.find_closest_opponent(player)\n",
        "                    player_actions[player.username] = player.related_bot.act(player_info)\n",
        "\n",
        "        # process multiple frames if frame skipping is enabled\n",
        "        for _ in range(skip_count):\n",
        "            if game_over:\n",
        "                break\n",
        "\n",
        "            self.steps += 1\n",
        "\n",
        "            players_info = {}\n",
        "            alive_players = []\n",
        "\n",
        "            for player in self.players:\n",
        "                ...\n",
        "                # handles action and movement logic (not needed to modify)\n",
        "\n",
        "                player_info = player.get_info()\n",
        "                player_info[\"shot_fired\"] = actions.get(\"shoot\", False)\n",
        "                player_info[\"closest_opponent\"] = self.find_closest_opponent(player)\n",
        "                players_info[player.username] = player_info\n",
        "\n",
        "            new_dic = {\n",
        "                \"general_info\": {\n",
        "                    \"total_players\": len(self.players),\n",
        "                    \"alive_players\": len(alive_players)\n",
        "                },\n",
        "                \"players_info\": players_info\n",
        "            }\n",
        "\n",
        "            final_info = new_dic\n",
        "\n",
        "            if len(alive_players) == 1:\n",
        "                ...\n",
        "                # game over condition handling\n",
        "                game_over = True\n",
        "                break\n",
        "\n",
        "        ...\n",
        "        # rendering and display update code skipped in training mode\n",
        "\n",
        "        if game_over:\n",
        "            print(\"Total steps:\", self.steps)\n",
        "            return True, final_info\n",
        "        else:\n",
        "            return False, final_info\n",
        "\n",
        "        # frame skipping for training acceleration\n",
        "        skip_count = self.frame_skip if self.training_mode else 1\n",
        "\n",
        "        # placeholder for the variables\n",
        "        game_over = False\n",
        "        final_info = None\n",
        "\n",
        "        # get actions once and reuse them for all skipped frames\n",
        "        player_actions = {}\n",
        "        if self.training_mode:\n",
        "            for player in self.players:\n",
        "                if player.alive:\n",
        "                    player_info = player.get_info()\n",
        "                    player_info['closest_opponent'] = self.find_closest_opponent(player)\n",
        "                    player_actions[player.username] = player.related_bot.act(player_info)\n",
        "\n",
        "        # process multiple frames if frame skipping is enabled\n",
        "        for _ in range(skip_count):\n",
        "            if game_over:\n",
        "                break\n",
        "\n",
        "            self.steps += 1\n",
        "\n",
        "            players_info = {}\n",
        "            alive_players = []\n",
        "\n",
        "            for player in self.players:\n",
        "                player.update_tick()\n",
        "\n",
        "                # use stored actions if in training mode with frame skipping\n",
        "                if self.training_mode and skip_count > 1:\n",
        "                    actions = player_actions.get(player.username, {})\n",
        "                else:\n",
        "                    # update info with closest opponent before getting action\n",
        "                    player_info = player.get_info()\n",
        "                    player_info['closest_opponent'] = self.find_closest_opponent(player)\n",
        "                    actions = player.related_bot.act(player_info)\n",
        "\n",
        "                if player.alive:\n",
        "                    alive_players.append(player)\n",
        "                    player.reload()\n",
        "\n",
        "                    # skip drawing in training mode for better performance\n",
        "                    if not self.training_mode:\n",
        "                        player.draw(self.world_surface)\n",
        "\n",
        "                    if debugging:\n",
        "                        print(\"Bot would like to do:\", actions)\n",
        "                    if actions.get(\"forward\", False):\n",
        "                        player.move_in_direction(\"forward\")\n",
        "                    if actions.get(\"right\", False):\n",
        "                        player.move_in_direction(\"right\")\n",
        "                    if actions.get(\"down\", False):\n",
        "                        player.move_in_direction(\"down\")\n",
        "                    if actions.get(\"left\", False):\n",
        "                        player.move_in_direction(\"left\")\n",
        "                    if actions.get(\"rotate\", 0):\n",
        "                        player.add_rotate(actions[\"rotate\"])\n",
        "                    if actions.get(\"shoot\", False):\n",
        "                        player.shoot()\n",
        "\n",
        "                    if not self.training_mode:\n",
        "                        # store position for trail\n",
        "                        if not hasattr(player, 'previous_positions'):\n",
        "                            player.previous_positions = []\n",
        "                        player.previous_positions.append(player.rect.center)\n",
        "                        if len(player.previous_positions) > 10:\n",
        "                            player.previous_positions.pop(0)\n",
        "\n",
        "                player_info = player.get_info()\n",
        "                player_info[\"shot_fired\"] = actions.get(\"shoot\", False)\n",
        "                player_info[\"closest_opponent\"] = self.find_closest_opponent(player)\n",
        "                players_info[player.username] = player_info\n",
        "\n",
        "            new_dic = {\n",
        "                \"general_info\": {\n",
        "                    \"total_players\": len(self.players),\n",
        "                    \"alive_players\": len(alive_players)\n",
        "                },\n",
        "                \"players_info\": players_info\n",
        "            }\n",
        "\n",
        "            # store the final state\n",
        "            final_info = new_dic\n",
        "\n",
        "            # check if game is over\n",
        "            if len(alive_players) == 1:\n",
        "                print(\"Game Over, winner is:\", alive_players[0].username)\n",
        "                if not self.training_mode:\n",
        "                    if self.use_advanced_UI:\n",
        "                        self.advanced_UI.display_winner_screen(alive_players)\n",
        "                    else:\n",
        "                        self.screen.fill(\"green\")\n",
        "\n",
        "                game_over = True\n",
        "                break\n",
        "\n",
        "        # skip all rendering operations in training mode for better performance\n",
        "        if not self.training_mode:\n",
        "            if self.use_advanced_UI:\n",
        "                self.advanced_UI.draw_everything(final_info, self.players, self.obstacles)\n",
        "            else:\n",
        "                # draw obstacles manually if not using advanced UI\n",
        "                for obstacle in self.obstacles:\n",
        "                    obstacle.draw(self.world_surface)\n",
        "\n",
        "            # scale and display the world surface\n",
        "            scaled_surface = pygame.transform.scale(self.world_surface, (self.display_width, self.display_height))\n",
        "            self.screen.blit(scaled_surface, (0, 0))\n",
        "            pygame.display.flip()\n",
        "\n",
        "        # in training mode, use a high tick rate but not unreasonably high\n",
        "        if not self.training_mode:\n",
        "            self.clock.tick(120)  # normal gameplay speed\n",
        "        else:\n",
        "            # skip the clock tick entirely in training mode for maximum speed\n",
        "            pass  # no tick limiting in training mode for maximum speed\n",
        "\n",
        "        # return the final state\n",
        "        if game_over:\n",
        "            print(\"Total steps:\", self.steps)\n",
        "            return True, final_info  # Game is over\n",
        "        else:\n",
        "            # return the final state from the last frame\n",
        "            return False, final_info\n",
        "\n",
        "    def find_closest_opponent(self, player):\n",
        "        ...\n",
        "        # returns the closest enemy location for strategic decisions\n",
        "\n",
        "\n",
        "    \"\"\"TO MODIFY\"\"\"\n",
        "    def calculate_reward_empty(self, info_dictionary, bot_username):\n",
        "        \"\"\"THIS FUNCTION IS USED TO CALCULATE THE REWARD FOR A BOT\"\"\"\n",
        "        \"\"\"NEEDS TO BE WRITTEN BY YOU TO FINE TUNE YOURS\"\"\"\n",
        "\n",
        "        # retrieve the players' information from the dictionary\n",
        "        players_info = info_dictionary.get(\"players_info\", {})\n",
        "        bot_info = players_info.get(bot_username)\n",
        "\n",
        "        # if the bot is not found, return a default reward of 0\n",
        "        if bot_info is None:\n",
        "            print(\"Bot not found in the dictionary\")\n",
        "            return 0\n",
        "\n",
        "        # extract variables from the bot's info\n",
        "        location = bot_info.get(\"location\", [0, 0])\n",
        "        rotation = bot_info.get(\"rotation\", 0)\n",
        "        rays = bot_info.get(\"rays\", [])\n",
        "        current_ammo = bot_info.get(\"current_ammo\", 0)\n",
        "        alive = bot_info.get(\"alive\", False)\n",
        "        kills = bot_info.get(\"kills\", 0)\n",
        "        damage_dealt = bot_info.get(\"damage_dealt\", 0)\n",
        "        meters_moved = bot_info.get(\"meters_moved\", 0)\n",
        "        total_rotation = bot_info.get(\"total_rotation\", 0)\n",
        "        health = bot_info.get(\"health\", 0)\n",
        "\n",
        "        # calculate reward:\n",
        "        reward = 0\n",
        "        # add your reward calculation here\n",
        "\n",
        "        # EXAMPLE\n",
        "        # Damage taken penalty - encourage defensive play\n",
        "        delta_health = self.last_health[bot_username] - health\n",
        "        if delta_health > 0:\n",
        "            reward -= delta_health * 0.2\n",
        "\n",
        "        return reward"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DZl3cnnTWJYa"
      },
      "source": [
        "## Key Concepts\n",
        "\n",
        "### Players and Bots:\n",
        "- `players`: are the active agents in the environment.\n",
        "- `bots`: are the AI controlling those players. The `act()` function is called each frame.\n",
        "\n",
        "### Reward Function Tracking:\n",
        "To calculate custom rewards, the environment stores, many variables, such as:\n",
        "- `last_positions`: for measuring movement.\n",
        "- `last_damage`, `last_kills`, `last_health`: to compute changes over time.\n",
        "- `visited_areas`: encourages exploration.\n",
        "\n",
        "the full list of variables can be found below\n",
        "\n",
        "### But what is Frame Skipping:\n",
        "- **Frame skipping** allows your model to **repeat the same action** for several frames.\n",
        "- It speeds up training by reducing the number of decisions made.\n",
        "- Set via `frame_skip` parameter.\n",
        "\n",
        "## IMPORTANT NOTE\n",
        "You can only use the variables given in the bot_info dictionary, modifing the contents of the dictionary will result in your bot being disqualified, you are supposed to use that data to generate your own mesurements."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jP5qzE_XZnB_"
      },
      "source": [
        "# Full list of variables in the bot_info dictionary\n",
        "\n",
        "\n",
        "Each bot receives a dictionary called `bot_info` at every step. This contains all the relevant data the environment collected about the bot, which can be used for decision-making and reward calculation.\n",
        "\n",
        "| Key | Type | Description |\n",
        "|-----|------|-------------|\n",
        "| `location` | List `[x, y]` | The current position of the bot in the game world. |\n",
        "| `rotation` | Float | The direction the bot is facing (in degrees or radians, depending on implementation). |\n",
        "| `rays` | List | Distance readings from raycast sensors (used for detecting nearby obstacles or enemies). |\n",
        "| `current_ammo` | Int | The current number of bullets or shots the bot has. |\n",
        "| `alive` | Bool | Whether the bot is still alive (`True`) or has been eliminated (`False`). |\n",
        "| `kills` | Int | How many opponents this bot has eliminated so far. |\n",
        "| `damage_dealt` | Float | Total damage this bot has dealt to other players. |\n",
        "| `meters_moved` | Float | The distance the bot has moved since the last step or over the whole game (depends on implementation). |\n",
        "| `total_rotation` | Float | How much the bot has rotated over time – can be used to detect erratic spinning. |\n",
        "| `health` | Int or Float | Current health level (usually 0 to 100). |\n",
        "| `shot_fired` | Bool | Whether the bot tried to shoot in this step. |\n",
        "| `closest_opponent` | List `[x, y]` | The position of the nearest living opponent. Useful for aiming or decision-making. |\n",
        "\n",
        "You can use any combination of these values in your custom `calculate_reward()` function.\n",
        "\n",
        "---\n",
        "**Examples of what you might do with this:**\n",
        "- Give positive reward for `damage_dealt` increase.\n",
        "- Give a small penalty for `shot_fired` without any damage.\n",
        "- Reward exploring new `location`s.\n",
        "- Encourage killing (`kills`) or discourage unnecessary rotation (`total_rotation`).\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MQsKZfXQcO0Q"
      },
      "source": [
        "# Bot example"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NesBdFsXcOdT"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import random\n",
        "\n",
        "class SimpleNN(nn.Module):\n",
        "    def __init__(self, input_size, output_size):\n",
        "        super(SimpleNN, self).__init__()\n",
        "        self.fc1 = nn.Linear(input_size, 64)\n",
        "        self.fc2 = nn.Linear(64, 32)\n",
        "        self.output = nn.Linear(32, output_size)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = F.relu(self.fc1(x))\n",
        "        x = F.relu(self.fc2(x))\n",
        "        return torch.sigmoid(self.output(x))  # Output between 0 and 1\n",
        "\n",
        "class MyBot:\n",
        "    def __init__(self):\n",
        "        self.model = SimpleNN(input_size=6, output_size=5)  # 6 inputs, 5 possible actions\n",
        "        self.model.eval()  # We are not training here\n",
        "\n",
        "    def act(self, player_info):\n",
        "        # Create input vector\n",
        "        x, y = player_info[\"location\"]\n",
        "        health = player_info[\"health\"]\n",
        "        ammo = player_info[\"current_ammo\"]\n",
        "        enemy_x, enemy_y = player_info[\"closest_opponent\"]\n",
        "\n",
        "        input_vector = torch.tensor([x, y, health, ammo, enemy_x, enemy_y], dtype=torch.float32)\n",
        "\n",
        "        # Forward pass through the model\n",
        "        with torch.no_grad():\n",
        "            output = self.model(input_vector)\n",
        "\n",
        "        # Convert output to boolean actions\n",
        "        # Output nodes: [forward, left, right, rotate, shoot]\n",
        "        actions = {\n",
        "            \"forward\": output[0].item() > 0.5,\n",
        "            \"left\": output[1].item() > 0.5,\n",
        "            \"right\": output[2].item() > 0.5,\n",
        "            \"rotate\": (output[3].item() - 0.5) * 2,  # range: [-1, 1]\n",
        "            \"shoot\": output[4].item() > 0.5\n",
        "        }\n",
        "\n",
        "        return actions"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1f_ksOoxfUCy"
      },
      "source": [
        "## How This Works\n",
        "\n",
        "1. The `SimpleNN` is a basic neural network with:\n",
        "   - 6 input features (x, y, health, ammo, enemy_x, enemy_y)\n",
        "   - 5 output actions: forward, left, right, rotate, shoot\n",
        "\n",
        "2. The `MyBot` class:\n",
        "   - Uses the NN to take `player_info` and compute actions.\n",
        "   - Converts model outputs to booleans (e.g. shoot if value > 0.5).\n",
        "   - Outputs a dictionary like `{\"forward\": True, \"shoot\": False}`.\n",
        "\n",
        "3. You can **train** this model using reinforcement learning later by:\n",
        "   - Storing `(state, action, reward, next_state)` tuples.\n",
        "   - Applying an RL algorithm like DQN."
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.13"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
